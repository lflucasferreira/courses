![Stanford University](https://www.abaenglish.com/es/wp-content/uploads/sites/2/2018/01/Logo-university-of-stanford.png)

# Machine Learning
> This is a 55 hours course 100% online offered by Stanford University

## Syllabus

#### Week 1
* ##### Introduction
> - [ ] Welcome to Machine Learning!
> - [ ] Welcome
> - [ ] What is Machine Learning?
> - [ ] Supervised Learning
> - [ ] Unsupervised Learning
* ##### Linear Regression with One Variable
> - [ ] Model Representation
> - [ ] Cost Function
> - [ ] Cost Function - Intuition I
> - [ ] Cost Function - Intuition II
> - [ ] Gradient Descent
> - [ ] Gradient Descent Intuition
> - [ ] Gradient Descent For Linear Regression
* ##### Linear Algebra Review
> - [ ] Matrices and Vectors
> - [ ] Addition and Scalar Multiplication
> - [ ] Matrix Vector Multiplication
> - [ ] Matrix Matrix Multiplication
> - [ ] Matrix Multiplication Properties
> - [ ] Inverse and Transpose

*Grade:* **pending**

---

#### Week 2
* ##### Linear Regression with Multiple Variables
> - [ ] Multiple Features
> - [ ] Gradient Descent for Multiple Variables
> - [ ] Gradient Descent in Practice I - Feature Scaling
> - [ ] Gradient Descent in Practice II - Learning Rate
> - [ ] Features and Polynomial Regression
> - [ ] Normal Equation1
> - [ ] Normal Equation Noninvertibility
> - [ ] Working on and Submitting Programming Assignments
* ##### Octave/Matlab Tutorial
> - [ ] Basic Operations
> - [ ] Moving Data Around
> - [ ] Computing on Data
> - [ ] Plotting Data
> - [ ] Control Statements: for, while, if statement
> - [ ] Vectorization

*Grade:* **pending**

---

#### Week 3
* ##### Logistic Regression
> - [ ] Classification
> - [ ] Hypothesis Representation
> - [ ] Decision Boundary
> - [ ] Cost Function
> - [ ] Simplified Cost Function and Gradient Descent
> - [ ] Advanced Optimization
> - [ ] Multiclass Classification: One-vs-all
* ##### Regularization
> - [ ] The Problem of Overfitting
> - [ ] Cost Function
> - [ ] Regularized Linear Regression
> - [ ] Regularized Logistic Regression

*Grade:* **pending**

---

#### Week 4
* ##### Neural Networks: Representation
> - [ ] Non-linear Hypotheses
> - [ ] Neurons and the Brain
> - [ ] Model Representation I
> - [ ] Model Representation II
> - [ ] Examples and Intuitions I
> - [ ] Examples and Intuitions II
> - [ ] Multiclass Classification

*Grade:* **pending**

---

#### Week 5
* ##### Neural Networks: Learning
> - [ ] Cost Function
> - [ ] Backpropagation Algorithm
> - [ ] Backpropagation Intuition
> - [ ] Implementation Note: Unrolling Parameters
> - [ ] Gradient Checking
> - [ ] Random Initialization
> - [ ] Putting It Together
> - [ ] Autonomous Driving

*Grade:* **pending**

---
    
#### Week 6
* ##### Advice for Applying Machine Learning
> - [ ] Deciding What to Try Next
> - [ ] Evaluating a Hypothesis
> - [ ] Model Selection and Train/Validation/Test Sets
> - [ ] Diagnosing Bias vs. Variance
> - [ ] Regularization and Bias/Variance
> - [ ] Learning Curves
> - [ ] Deciding What to Do Next Revisited
* ##### Machine Learning System Design
> - [ ] Prioritizing What to Work On
> - [ ] Error Analysis
> - [ ] Error Metrics for Skewed Classes
> - [ ] Trading Off Precision and Recall
> - [ ] Data For Machine Learning

*Grade:* **pending**

---

#### Week 7
* ##### Support Vector Machines
> - [ ] Optimization Objective
> - [ ] Large Margin Intuition
> - [ ] Mathematics Behind Large Margin Classification
> - [ ] Kernels I
> - [ ] Kernels II
> - [ ] Using An SVM

*Grade:* **pending**

---

#### Week 8
* ##### Unsupervised Learning
> - [ ] Unsupervised Learning: Introduction
> - [ ] K-Means Algorithm
> - [ ] Optimization Objective
> - [ ] Random Initialization
> - [ ] Choosing the Number of Clusters
* ##### Dimensionality Reduction
> - [ ] Motivation I: Data Compression
> - [ ] Motivation II: Visualization
> - [ ] Principal Component Analysis Problem Formulation
> - [ ] Principal Component Analysis Algorithm
> - [ ] Reconstruction from Compressed Representation
> - [ ] Choosing the Number of Principal Components
> - [ ] Advice for Applying PCA

*Grade:* **pending**

---

#### Week 9
* ##### Anomaly Detection
> - [ ] Problem Motivation
> - [ ] Gaussian Distribution
> - [ ] Algorithm
> - [ ] Developing and Evaluating an Anomaly Detection System
> - [ ] Anomaly Detection vs. Supervised Learning
> - [ ] Choosing What Features to Use
> - [ ] Multivariate Gaussian Distribution
> - [ ] Anomaly Detection using the Multivariate Gaussian Distribution
* ##### Recommender Systems
> - [ ] Problem Formulation
> - [ ] Content Based Recommendations
> - [ ] Collaborative Filtering
> - [ ] Collaborative Filtering Algorithm
> - [ ] Vectorization: Low Rank Matrix Factorization
> - [ ] Implementational Detail: Mean Normalization

*Grade:* **pending**

---
 
#### Week 10
* ##### Large Scale Machine Learning
> - [ ] Learning With Large Datasets
> - [ ] Stochastic Gradient Descent
> - [ ] Mini-Batch Gradient Descent
> - [ ] Stochastic Gradient Descent Convergence
> - [ ] Online Learning
> - [ ] Map Reduce and Data Parallelism

*Grade:* **pending**

---

#### Week 11
* ##### Application Example: Photo OCR
> - [ ] Problem Description and Pipeline
> - [ ] Sliding Windows
> - [ ] Getting Lots of Data and Artificial Data
> - [ ] Ceiling Analysis: What Part of the Pipeline to Work on Next
> - [ ] Summary and Thank You

*Grade:* **pending**

---

**Total Grade: pending**

## About this Course
> Machine learning is the science of getting computers to act without being explicitly programmed. In the past decade, machine learning has given us self-driving cars, practical speech recognition, effective web search, and a vastly improved understanding of the human genome. Machine learning is so pervasive today that you probably use it dozens of times a day without knowing it. Many researchers also think it is the best way to make progress towards human-level AI. In this class, you will learn about the most effective machine learning techniques, and gain practice implementing them and getting them to work for yourself. More importantly, you'll learn about not only the theoretical underpinnings of learning, but also gain the practical know-how needed to quickly and powerfully apply these techniques to new problems. Finally, you'll learn about some of Silicon Valley's best practices in innovation as it pertains to machine learning and AI.

> This course provides a broad introduction to machine learning, datamining, and statistical pattern recognition. Topics include: (i) Supervised learning (parametric/non-parametric algorithms, support vector machines, kernels, neural networks). (ii) Unsupervised learning (clustering, dimensionality reduction, recommender systems, deep learning). (iii) Best practices in machine learning (bias/variance theory; innovation process in machine learning and AI). The course will also draw from numerous case studies and applications, so that you'll also learn how to apply learning algorithms to building smart robots (perception, control), text understanding (web search, anti-spam), computer vision, medical informatics, audio, database mining, and other areas.

> https://www.coursera.org/learn/machine-learning

*Instructor:* **Andrew Ng**
